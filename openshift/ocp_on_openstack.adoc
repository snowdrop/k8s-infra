= OCP on OpenStack
Antonio Costa
:revdate: {docdate}
:icons: font
:toc: left
:toclevels: 3
:description: This document describes the installation procedure to deploy an OCP cluster on RHOS.

== Introduction

Using the Ansible OCP Cluster role, this document describes the procedure to deploy an OCP cluster on RHOS.

[glossary]
== Terminology

[glossary]
OCP:: OpenShift Container Platform
RHOS:: RedHat OpenStack

== Overview

The Ansible playbook and role used to install an OCP cluster on a RHOS cloud use the link:https://docs.openshift.com/container-platform/4.12/installing/index.html[OpenShift install] procedure for OpenStack. 

[NOTE]
====
More detailed information on the installation procedure is available in the link:https://docs.openshift.com/container-platform/4.12/installing/installing_openstack/preparing-to-install-on-openstack.html[RedHat OpenShift / Installing on OpenStack] page.
====

The early stages of the deployment process create an `install-config.yaml` file that is fed to the `openshift-install` process which then generates the RHOS VMs and deploys the OCP cluster on them.

.OpenShift Install installation folder
[WARNING]
====
The installation process generates an installation folder on the host machine
(by default is the same machine executing the Ansible playbooks) that must be
kept for maintenance purposes, e.g. removing the OCP cluster.

The 
====

== Prerequisites

The prerequisites for using RHOS Ansible Playbooks are the following.

._Click to open the details_
[%collapsible]
====

[]
======
include::../openstack/README.adoc[tag=rhos_prerequisites]
======

====

More information on the link:../openstack/README.adoc[OpenStack README] for this project.

== Deploy OCP cluster on RHOS

include::../ansible/playbook/ocp/README.adoc[tag=deploy_ocp_on_rhos]

=== Backup the installation directory

Generate the base64 codification of the `.tar.gz` file containing the backup of the `openshift-install` directory.

[source,bash]
----
base64  ocp-sscpc-data.tar.gz > ocp-sscpc-data.tar.gz.base64
----

Copy the contents of the file to the clipboard.

[source,bash]
----
xclip -sel c < /opt/ocp/ocp-sscpc-data.tar.gz.base64
----

Insert the previously copied contents as a `pass` entry.

[source,bash]
----
pass insert openstack/ocp-sscpc/install_dir -m
----

Push the `pass` changes to the git repository.

[source,bash]
----
git push
----


== Post installation steps


=== Install jump server

To be able to SSH to any of the OCP cluster machines a _jump server_ is required.

==== Requirements

1. Obtain the name of the network interface created by the OCP installation process
  * As a result of the OCP installation process on RHOS a network is created that allows communication between the cluster instances. 
  * The name should resemble `ocp-<code>-openshift` where `ocp` is the prefix our ansible role applies to the installation and `<code>` is a dynamic string generated by the installation process.

[NOTE]
====
For the sake of example the `ocp-sscpc` prefix will be used.
====

==== Create and configure the server

[WARNING]
====
One _jump server_ will be required so the OCP cluster name prefix should be used when creating this VM.
====

[source,bash]
----
ansible-playbook ansible/playbook/openstack/openstack_vm_create_passwordstore.yml \
  -e '{"openstack": {"vm": {"network": "ocp-sscpc-openshift","image": "Fedora-Cloud-Base-29", "flavor": "m1.small"}}}' \
  -e vm_name=ocp-sscpc-jump-server
----

Create a _Floating IP_ from the external network that one's router is connected to, in our case `provider_net_cci_13`. As the _jump server_ is created on the same network as the OCP cluster it isn't accesible from the outside. To access this server a RHOS floating IP is required. The `provider_net_cci_13` is the team's public RHOS network name.

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI.

[source,bash]
----
openstack floating ip create provider_net_cci_13
----
====

Associate the floating IP with the jump server.

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI.

[source,bash]
----
openstack server add floating ip <uuid_of_the_create_server> <floating_ip_that_you_got_from_previous_command>
----

====

Since the _Floating IP_ is the _public_ IP to access the server it must be added to the Passwordstore Ansible Inventory.

[WARNING]
====
Manually add an entry to the passwordstore under `openstack/ocp-sscpc-jump-server/floating_ip` with the floating IP. 
This will allow the `./tools/passstore-vm-ssh.sh` tool
to catch that variable as the 1st priority server IP address.
====

==== Copy the cluster SSH key

The OCP cluster has been generated using an SSH key. To connect to each of the OCP cluster machines this key is required.

Copy to the _jump server_ the ssh key used in the deployment of the OCP cluster.

[source,bash]
----
scp -i ${HOME}/.ssh/id_rsa_snowdrop_openstack id_rsa_snowdrop_openstack snowdrop@$(pass show ${VM_PROVIDER}/${VM_NAME}/floating_ip | awk 'NR==1{print $1}'):/home/snowdrop/.ssh/
----

Connect to the _jump server_.

[source,bash]
----
./tools/passstore-vm-ssh.sh openstack ocp-sscpc-jump-server
----

The server prompt should be presented.

[source]
----
[snowdrop@ocp-sscpc-jump-server ~]$
----

From there connect to the OCP machine.

[source,bash]
----
$ ssh -i ${HOME}/.ssh/id_rsa_snowdrop_openstack core@<ocp instance ip address>
----

This should result in successfull connection.

[source]
----
Red Hat Enterprise Linux CoreOS 412.86.202303141242-0
  Part of OpenShift 4.12, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html

----

== Remove existing OCP cluster on RHOS

include::../ansible/playbook/ocp/README.adoc[tag=undeploy_ocp_on_rhos]
