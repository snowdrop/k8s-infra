= OCP on OpenStack
Antonio Costa
:revdate: {docdate}
:icons: font
:toc: left
:toclevels: 3
:description: This document describes the installation procedure to deploy an OCP cluster on RHOS.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :bookmark_tabs:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Introduction

Using the Ansible OCP Cluster role, this document describes the procedure to deploy an OCP cluster on RHOS.

[glossary]
== Terminology

[glossary]
OCP:: OpenShift Container Platform
RHOS:: RedHat OpenStack

== Overview

The Ansible playbook and role used to install an OCP cluster on a RHOS cloud use the link:https://docs.openshift.com/container-platform/4.12/installing/index.html[OpenShift install] procedure for OpenStack. 

[NOTE]
====
More detailed information on the installation procedure is available in the link:https://docs.openshift.com/container-platform/4.12/installing/installing_openstack/preparing-to-install-on-openstack.html[RedHat OpenShift / Installing on OpenStack] page.
====

The early stages of the deployment process create an `install-config.yaml` file that is fed to the `openshift-install` process which then generates the RHOS VMs and deploys the OCP cluster on them.

.OpenShift Install installation folder
[WARNING]
====
The installation process generates an installation folder on the host machine
(by default is the same machine executing the Ansible playbooks) that must be
kept for maintenance purposes, e.g. removing the OCP cluster.

The 
====

== Prerequisites

The prerequisites for using RHOS Ansible Playbooks are the following.

._Click to open the details_
[%collapsible]
====

[]
======
include::../openstack/README.adoc[tag=rhos_prerequisites]
======

====

More information on the link:../openstack/README.adoc[OpenStack README] for this project.

== Deploy OCP cluster on RHOS

include::../ansible/playbook/ocp/README.adoc[tag=deploy_ocp_on_rhos]

=== Backup the installation directory

Generate the base64 codification of the `.tar.gz` file containing the backup of the `openshift-install` directory.

[source,bash]
----
base64  ocp-sscpc-data.tar.gz > ocp-sscpc-data.tar.gz.base64
----

Copy the contents of the file to the clipboard.

[source,bash]
----
xclip -sel c < /opt/ocp/ocp-sscpc-data.tar.gz.base64
----

Insert the previously copied contents as a `pass` entry.

[source,bash]
----
pass insert openstack/ocp-sscpc/install_dir -m
----

Push the `pass` changes to the git repository.

[source,bash]
----
git push
----

== Post installation steps

=== Install jump server

The OCP cluster is created on a private network that allows communication 
 between the different hosts of the cluster. To be able to SSH to any of 
 the hosts of this private network a _jump server_ host is required. 
 This host must be created inside the network of the OCP cluster.
 
To access the _jump server_ a Public IP is required which can be 
 accomplished by creating a RHOS _Floating IP_ and associating it with that 
 host.

The OCP private network is created automatically during the installation 
 process. The name of this network is built using the `clusterName` defined 
 in the installation process, a `<code>` that is a dynamic string generated by the installation process 
 and the `openshift` suffix (`<clustername>-<code>-openshift`).

==== Requirements

*Collect the metadata of the installation executed previously.*

The metadata of the installation can be obtained from the `metadata.json` 
 file of the OCP installation directory.

.Sample metadata.json
[source,json]
----
{
  "clusterName": "ocp",
  "clusterID": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "infraID": "ocp-sscpc",
  "openstack": {
    "cloud": "openstack",
    "identifier": {
      "openshiftClusterID": "ocp-sscpc"
    }
  }
}
----

[NOTE]
====
For the sake of example the `ocp-sscpc` prefix will be used.

Also, the calculated RHOS internal network name should be  
 `ocp-sscpc-openshift`.
====

==== Create and configure the server

[WARNING]
====
Since each OCP cluster generates it's own network one _jump server_ will be 
 required for each cluster. As a consequence the name of the_jump server_ 
 should include the prefix of the cluster (`<clusterName>-<code>`) as identified in the metadata of the OCP cluster.
====

[source,bash]
----
ansible-playbook ansible/playbook/openstack/openstack_vm_create_passwordstore.yml \
  -e '{"openstack": {"vm": {"network": "ocp-sscpc-openshift","image": "Fedora-Cloud-Base-29", "flavor": "m1.small"}}}' \
  -e vm_name=ocp-sscpc-jump-server
----

Once the host is provisioned create a _Floating IP_ from the external 
 network that one's router is connected to. 
 
To collect the name of the 
 _External Network_ go to the `Network > Routers` option of the RHOS console 
 and check the `External Network` column for the OCP cluster router. The 
 OCP cluster router can be identified by it's name that should be 
 `<clusterName>-<code>-external-router`. In this example `ocp-sscpc-external-router`.

The `ocp_cluster`Ansible Role as this value predefined in the installation 
procedure and it's `provider_net_cci_13`. It's defined 
 link:../ansible/roles/ocp_cluster/defaults/main.yml[here]. 

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI. The RHOS console option is `Network > Floating IPs`.

.Create Floating IP using openstack CLI
[source,bash]
----
openstack floating ip create provider_net_cci_13
----
====

Associate the floating IP with the jump server.

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI. The RHOS console option is the `Associate` button on the `Network > Floating IPs`.

.Associate a Floating IP using openstack CLI
[source,bash]
----
openstack server add floating ip <uuid_of_the_create_server> <floating_ip_that_you_got_from_previous_command>
----
====

Since the _Floating IP_ is the _public_ IP to access the server it must be added to the Passwordstore Ansible Inventory.

[WARNING]
====
Manually add an entry to the passwordstore under `openstack/ocp-sscpc-jump-server/floating_ip` with the floating IP. 
This will allow the `./tools/passstore-vm-ssh.sh` tool
to catch that variable as the 1st priority server IP address.
====

[WARNING]
====
ATTOW the Ansible Passwordstore implementation isn't taking the 
 `floating_ip` into considereation ([.yellow]#*TODO*#).
====

==== Copy the cluster SSH key

The OCP cluster has been generated using an SSH key. To connect to each of the OCP cluster machines this key is required.

Copy to the _jump server_ the ssh key used in the deployment of the OCP cluster.

[source,bash]
----
scp -i ${HOME}/.ssh/id_rsa_snowdrop_openstack id_rsa_snowdrop_openstack snowdrop@$(pass show ${VM_PROVIDER}/${VM_NAME}/floating_ip | awk 'NR==1{print $1}'):/home/snowdrop/.ssh/
----

Connect to the _jump server_.

[source,bash]
----
./tools/passstore-vm-ssh.sh openstack ocp-sscpc-jump-server
----

The server prompt should be presented.

[source]
----
[snowdrop@ocp-sscpc-jump-server ~]$
----

From there connect to the OCP machine.

[source,bash]
----
$ ssh -i ${HOME}/.ssh/id_rsa_snowdrop_openstack core@<ocp instance ip address>
----

This should result in successfull connection.

[source]
----
Red Hat Enterprise Linux CoreOS 412.86.202303141242-0
  Part of OpenShift 4.12, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html

----

== Remove existing OCP cluster on RHOS

include::../ansible/playbook/ocp/README.adoc[tag=undeploy_ocp_on_rhos]
