= OCP on OpenStack
Antonio Costa
:revdate: {docdate}
:icons: font
:toc: left
:toclevels: 3
:description: This document describes the installation procedure to deploy an OCP cluster on RHOS.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :bookmark_tabs:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Introduction

Using the Ansible OCP Cluster role, this document describes the procedure to deploy an OCP cluster on RHOS.

[glossary]
== Terminology

[glossary]
OCP:: OpenShift Container Platform
RHOS:: RedHat OpenStack

== Overview

The Ansible playbook and role used to install an OCP cluster on a RHOS cloud use the link:https://docs.openshift.com/container-platform/4.12/installing/index.html[OpenShift install] procedure for OpenStack. 

[NOTE]
====
More detailed information on the installation procedure is available in the link:https://docs.openshift.com/container-platform/4.12/installing/installing_openstack/preparing-to-install-on-openstack.html[RedHat OpenShift / Installing on OpenStack] page.
====

The early stages of the deployment process create an `install-config.yaml` file that is fed to the `openshift-install` process which then generates the RHOS VMs and deploys the OCP cluster on them.

.OpenShift Install installation folder
[WARNING]
====
The installation process generates an installation folder on the host machine
(by default is the same machine executing the Ansible playbooks) that must be
kept for maintenance purposes, e.g. removing the OCP cluster.
====

To collect information required to the process, and also store the results of the installation, passwordstore is used (check more information link:../passwordstore/README.adoc[here]).

== Requirements

The prerequisites for using the OCP on RHOS Ansible Playbooks are described on the link:../requirements.txt[`requirements.txt`] and link:../collections/requirements.yml[`collections/requirements.yml`] files. 

More information on our link:../openstack/README.adoc[OpenStack README] for this project.

Python virtual environment can be used to isolate all the python requirements from the host OS. For more information check the link:../ansible/README.adoc#python-venv[Python Virtual Env] section on our Ansible README.

=== Passwordstore

Passwordstore is used to both provide information to the playbooks as well 
 as store the result of the installation process. 

=== Sizing

The sizing of the OCP cluster is done by selecting the RHOS flavor of the
 master and worker nodes as well as the number of replicas for each.

For more information on obtaining Flavor information from RHOS using the 
 CLI check our link:../openstack/openstack-cli.adoc[RHOS CLI] document.

More information on the following links:

* link:https://docs.openshift.com/container-platform/4.12/installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html[Setting up the environment for an OpenShift installation]
* link:https://docs.openshift.com/container-platform/4.12/installing/installing_openstack/installing-openstack-user.html[Installing a cluster on OpenStack on your own infrastructure]

== Installation process

=== Deploy OCP cluster on RHOS

Using the `openshift-install` tool deploy the OCP cluster on the RHOS infrastructure.

Check the link:../ansible/playbook/ocp/README.adoc[OCP README] document on the link:../ansible/playbook/ocp[OCP PlayBooks] folder.

=== Backup the installation directory

After the cluster deployment is finished let's backup the installation directory.

[CAUTION]
====
As referred prior on this document, the installation folder must be kept for 
 correct server maintenance. The procedure to make this backup is the following.
====

Generate the base64 codification of the `.tar.gz` file containing the backup of the `openshift-install` directory.

[source,bash]
----
base64  ocp-sscpc-data.tar.gz > ocp-sscpc-data.tar.gz.base64
----

Copy the contents of the file to the clipboard.

[source,bash]
----
xclip -sel c < /opt/ocp/ocp-sscpc-data.tar.gz.base64
----

Insert the previously copied contents as a `pass` entry.

[source,bash]
----
pass insert openstack/ocp-sscpc/install_dir -m
----

Push the `pass` changes to the git repository.

[source,bash]
----
git push
----

== Post installation steps

=== Install jump server

An OCP cluster, when created on RHOS", is deployed on a private OpenStack 
 network that allows communication 
 between the different hosts of the cluster. To be able to SSH to any of 
 the hosts of this private network a _jump server_ host is required. 
 This host must be created inside the network of the OCP cluster.
 
To access the _jump server_ a Public IP is required which can be 
 accomplished by creating a RHOS _Floating IP_ and associating it with that 
 host.

The private network is created automatically during the execution of the 
 OCP playbook creating the cluster on RHOS. 
 The name of this network is built using the `clusterName` defined 
 in the installation process, a `<code>` that is a dynamic string generated by the installation process 
 and the `openshift` suffix (`<clustername>-<code>-openshift`).

==== Requirements

*Collect the metadata of the installation executed previously.*

The metadata of the installation can be obtained from the `metadata.json` 
 file of the OCP installation directory.

.Sample metadata.json
[source,json]
----
{
  "clusterName": "ocp",
  "clusterID": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "infraID": "ocp-sscpc",
  "openstack": {
    "cloud": "openstack",
    "identifier": {
      "openshiftClusterID": "ocp-sscpc"
    }
  }
}
----

[NOTE]
====
For the sake of example the `ocp-sscpc` prefix will be used.

Also, the calculated RHOS internal network name should be  
 `ocp-sscpc-openshift`.
====

==== Create and configure the server

[WARNING]
====
Since each RHOS OCP cluster installation generates it's own RHOS network,  one _jump server_ will be required for each cluster. 
 As a consequence the name of the _jump server_ 
 should include the prefix of the cluster (`<clusterName>-<code>`) as identified in the metadata of the OCP cluster.
====

[source,bash]
----
ansible-playbook ansible/playbook/openstack/openstack_vm_create_passwordstore.yml \
  -e '{"openstack": {"vm": {"network": "ocp-sscpc-openshift","image": "Fedora-Cloud-Base-29", "flavor": "m1.small"}}}' \
  -e vm_name=ocp-sscpc-jump-server
----

Once the _jump server_ host is provisioned on RHOS, create a _Floating IP_ 
 to be able to access that host. This _Floating IP_ must be associated 
 with the RHOS _External network_ use by the OCP cluster hosts. This 
 _External network_ can be identified from the RHOS _Router_ genereated 
 by the RHOS OCP installation.
 
To collect the name of the _External Network_ go to the
 `Network > Routers` option of the RHOS console. 
 The RHOS router for the OCP cluster can be identified by it's name 
 that should be `<clusterName>-<code>-external-router`.
 Once the router is identified check the `External Network` column, which in this example would be `ocp-sscpc-external-router`.

[NOTE]
====
The value of the _External Network_ is provided during the installation 
 process. If not overriden it defaults to the value defined at the 
 link:../ansible/roles/ocp_cluster/defaults/main.yml[`ocp_cluster` Ansible Role].

.`ocp_cluster` default value for the variable _External Network_
[source,yaml]
----
include::../ansible/roles/ocp_cluster/defaults/main.yml[tag=rhos_default_network_provider]
----

====

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI. The RHOS console option is `Network > Floating IPs`.

.Create Floating IP using openstack CLI
[source,bash]
----
openstack floating ip create provider_net_cci_13
----
====

Associate the floating IP with the jump server.

[NOTE]
====
TBD: An ansible playbook that implements this.

For now this needs to be done using the RHOS console or using the `openstack` CLI. The RHOS console option is the `Associate` button on the `Network > Floating IPs`.

.Associate a Floating IP using openstack CLI
[source,bash]
----
openstack server add floating ip <uuid_of_the_create_server> <floating_ip_that_you_got_from_previous_command>
----
====

Since the _Floating IP_ is the _public_ IP to access the server it must be added to the Passwordstore Ansible Inventory.

[WARNING]
====
Manually add an entry to the passwordstore under `openstack/ocp-sscpc-jump-server/floating_ip` with the floating IP. 
This will allow the `./tools/passstore-vm-ssh.sh` tool
to catch that variable as the 1st priority server IP address.
====

[WARNING]
====
ATTOW the Ansible Passwordstore implementation isn't taking the 
 `floating_ip` into considereation ([.yellow]#*TODO*#).
====

==== Copy the cluster SSH key

The OCP cluster has been generated using an SSH key. To connect to each of the OCP cluster machines this key is required.

Copy to the _jump server_ the ssh key used in the deployment of the OCP cluster.

[source,bash]
----
scp -i ${HOME}/.ssh/id_rsa_snowdrop_openstack id_rsa_snowdrop_openstack snowdrop@$(pass show ${VM_PROVIDER}/${VM_NAME}/floating_ip | awk 'NR==1{print $1}'):/home/snowdrop/.ssh/
----

Connect to the _jump server_.

[source,bash]
----
./tools/passstore-vm-ssh.sh openstack ocp-sscpc-jump-server
----

The server prompt should be presented.

[source]
----
[snowdrop@ocp-sscpc-jump-server ~]$
----

From there connect to the OCP machine.

[source,bash]
----
$ ssh -i ${HOME}/.ssh/id_rsa_snowdrop_openstack core@<ocp instance ip address>
----

This should result in successfull connection.

[source]
----
Red Hat Enterprise Linux CoreOS 412.86.202303141242-0
  Part of OpenShift 4.12, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html

----

== Remove existing OCP cluster on RHOS

include::../ansible/playbook/ocp/README.adoc[tag=undeploy_ocp_on_rhos]
