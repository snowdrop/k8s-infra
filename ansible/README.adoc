= Ansible
:icons: font
:toc: left
:description: This document introduces some of the key concepts that you should be aware when you play with Ansible in order to configure the environment to let Ansible to access the different machines.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Conventions

When we refer to the `controller`, that means that we speak about the machine that executes the `Ansible playbooks` and `roles`
and which owns the inventory. The `hosts` are the machines where the playbooks and roles are executed against. 

The exception goes to the playbooks that are executed against `localhost`. This special host is referred to the `controller`. 

== Installation guide

In order to play with the playbooks/roles of this project, it is needed to:

* Install Ansible 
** Preferrably by using the <<Python Virtual Environments>> method
** For more installation check the https://docs.ansible.com/ansible/latest/installation_guide/index.html[Ansible Installation Guide]
** Ansible uses Python, so check the Ansible https://docs.ansible.com/ansible/latest/reference_appendices/python_3_support.html[Python 3 Support] page for more information.
* Install https://www.passwordstore.org/[`passwordstore`]
** This project uses [pass](https://www.passwordstore.org/) to store passwords and certificates related to the project itself.
** More information on installing pass in the Download section of the [passwordstore web page](https://www.passwordstore.org/)
* Clone the `passstore` project from GitLab.
* Install the `snowdrop/cloud_infra`` Ansible Collection


> **NOTE**: Since passwordstore is integrated with [git](https://git-scm.com/), all changes made locally to a pass repository are automatically committed to the local git repo.
> Don't forget to `git push` and `git pull` often in order to have your local repository synchronized with other team members as well as publishing to the team your changes. 

=== Python Virtual Environments

This project suggests using a link:https://docs.python.org/3/library/venv.html[python virtual environment] to manage all the 
python dependencies.

From the `k8s-infra` root folder create a virtual environment. This 
section describes the procedure to create a virtual environment for using with the project.

First create the `.snowdrop-venv` virtual environment.

[NOTE]
====
This command only needs to be executed the first time. After
being created the virtual environment status will be stored at 
the `.snowdrop-venv` folder at the root of the project. This
folder has been excluded from git.
====

.Create the `.snowdrop-venv` virtual environment
[source,bash]
----
python3 -m venv .snowdrop-venv
----

Then activate the virtual environment to be able to use it. Activating is done by executing the following command.

.Activate `.snowdrop-venv` virtual environment
[source,bash]
----
source .snowdrop-venv/bin/activate
----

After this the virtual environment name will show up on the bash prompt (e.g. `(.snowdrop-venv) [janedoe@mycomputer k8s-infra]$`).

Once using the python virtual environment install the requirements.

[NOTE]
====
This command only needs to be executed the first time. 
====

.Install the python requirements
[source,bash]
----
python3 -m pip install -r requirements.txt
----

ansible-galaxy install -r requirements.yml

To deactivate the python virtual environment execute the following command.

[source,bash]
----
deactivate
----

As a result the `.snowdrop-venv` reference on the bash prompt will disappear.

References:

* https://www.ansible.com/blog/developing-and-testing-ansible-roles-with-molecule-and-podman-part-1
* https://www.ansible.com/blog/developing-and-testing-ansible-roles-with-molecule-and-podman-part-2

=== Install the snowdrop.cloud_infra Ansible Collection

Requires: Python Virtual Environment active.

Go to the `snowdrop.cloud_infra` collection folder.

[source,bash]
----
cd ansible/ansible_collections/snowdrop/cloud_infra
----

Build the collection.

[source,bash]
----
ansible-galaxy collection build ansible/ansible_collections/snowdrop/cloud_infra --output build/ --force
----

This will generate a `.tar.gz` file with name `snowdrop-cloud_infra-<version>.tar.gz` on the link:./build/[] subfolder of the `k8s-infra` repository.

Finally install the collection.

[source,bash]
----
ansible-galaxy collection install ./build/snowdrop-cloud_infra-1.0.0.tar.gz --upgrade
----

== User Guide

Provisioning and accessing a server requires several steps, each of which will be covered in this section.

1. Manage Ansible Inventory groups [...](<<Group management in ansible>>)
1. Manage Ansible Inventory host records [...](#host-management-in-ansible)
1. Create Server in Cloud Provider [...](<<Create server>>)
1. Secure Host [...](<<Secure host>>)
1. Install Software [...](<<Install software>>)

=== Group management in ansible

Groups are defined in the `hosts.yml` file that exists in the inventory folder [../inventory/hosts.yml](../inventory/hosts.yml). The 2 main goals of groups is to apply variable values and filter playbook execution. 

There can are different groups for the providers, ATTOW only one exists which is hetzner. The goal is one and only to fill the provider variable.

```yaml
all:
  children:
    hetzner:
      vars:
        pass_provider: hetzner
```

Another existing group is `k8s` are associated with kubernetes and assign the kubernetes version or ports to be open.

```yaml
all:
  children:
...
    k8s:
      children:
```

For instance, kubernetes group for version 1.15 defines the following variables:

```yaml
        k8s_115:
          vars:
            k8s_version: 1.15.9
            k8s_dashboard_version: v2.0.0-rc5
            coreos_flannel_sha_commit: a70459be0084506e4ec919aa1c114638878db11b
```

If it would be required to support a new kubernetes version than a new set of variables should be added to the inventory file. For instance, preparing the 
installation for version `1.17` would require adding a new group as child of the `k8s` group. It might also be required to adjust the dashboard and flannel values. 

```yaml
        k8s_117:
          vars:
            k8s_version: 1.17.4
            k8s_dashboard_version: v2.0.0-rc5
            coreos_flannel_sha_commit: a70459be0084506e4ec919aa1c114638878db11b
``` 

=== Host management in Ansible

The first step is to add the host to the Ansible inventory but also to create the needed keys under the password store. This section describes how to maintain our hosts and their use.

==== Updating and retrieving the inventory

As commented before, the host information (user, pwd, ssh port, ...) is obtained from the github `passwordstore` team [project](https://github.com/snowdrop/pass). 

Because a host can already be defined under the store, prior to execute the playbook creating a host, check the content of the store using the following command

```bash
$ pass hetzner
hetzner
├── ...
├── host-1
│   ├── ...
├── host-2
│   ├── ...
```

According to what you will find under the `Hetzner` level, then 2 scenario will take place:

1. The host exists. Jump to the [Import a host](#Import-a-host) section;
2. The host doesn't exist. Create a new host as documented under the section [Create a host](#Create-a-host)

> NOTE: Check the [team password store documentation](https://github.com/snowdrop/pass) if it is not yet installed on your laptop.
> 
> WARNING: Whenever a command to create a host and password entries took place, then push the content using the command `pass git push` manually !!

==== Import a host

If a host has already been created, it can be imported within the inventory using the command: 

```bash
$ ansible-playbook ansible/playbook/passstore_controller_inventory.yml -e vm_name=<VM_NAME> -e pass_provider=hetzner
```
where `<VM_NAME>` corresponds to the host key created under `hetzner`

**REMARK**: The playbook used is the same as the one described in the [Create a host](#Create-a-host) section but without the `create` *tag*.

==== Create a host

If the host doesn't exist it must be generated and added to the Ansible inventory. 

This is done using the `passstore_controller_inventory` playbook. More information on how to use this playbook in the [`passstore_controller_inventory` section](#passstore_controller_inventory).

> NOTE: ATTOW the only supported provider is `hetzner`. 

==== Remove a host 

This is done using the `passstore_controller_inventory_remove` playbook. More information on how to use this playbook in the [`passstore_controller_inventory_remove` section](#passstore_controller_inventory_remove).

```bash
$ ansible-playbook ansible/playbook/passstore_controller_inventory_remove.yml -e vm_name=<vm_name> -e pass_provider=<provider>
```

> NOTE: ATTOW the only provider tested is `hetzner`. 

=== Create Server

Once the inventory is defined the server can be provisioned, if it isn't.

There should be different playbooks for each of the providers so check the corresponding provider:

* [Hetzner](../../hetzner/README-cloud.md)

Once the server is created it must be securized. More information on the next section.

=== Secure host

Host securization is of utmost importance. For this reason a specific playbook and roles have been generated to perform this task.

For the execution of the securization check the [`sec_host` playbook section](#sec_host).  

=== Install software

==== k8s

For information on k8s playbooks and roles check ../../kubernetes/README.md[here].

== Folders

link:ansible_collections.adoc[Ansible Collections]: `ansible_collections`

link:ansible-inventory.adoc[Ansible Inventory]: `inventory`

link:playbook/README.adoc[Ansible Playbooks]: `playbook`

link:roles[Ansible Roles]: `roles`

== Development Guide

=== Testing

Testing 

[source,bash]
----
molecule test
----
