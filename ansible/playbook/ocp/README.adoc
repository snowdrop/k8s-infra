= OCP on RHOS Ansible Playbooks
Snowdrop Team
:icons: font
:revdate: {docdate}
:toc: left
:toclevels: 3
:description: Deploying OCP on RHOS
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Introduction

[.lead]
This document describes the process to deploy an OCP cluster on a 
 RHOS infrastructure.

The installation process uses the _OpenShift Container Platform installer_ 
 obtained from https://mirror.openshift.com/.

== OCP On OpenStack

Playbooks to deploy and remove an OCP cluster to RHOS. 

.List of OCP RHOS playbooks
[cols="30%m,70%"]
|===
|Playbook File |Description

| ocp_openstack_install.yml
| Deploy an OCP cluster on RHOS.

| ocp_openstack_remove.yml
| Remove an OCP cluster on RHOS.

| ocp_openstack_info.yml
a| Print information from the deployed OCP cluster.

This playbook will print cluster information such as Console URL, kubeadmin password, ...

| ocp_openstack_test.yml
a| Test some functionalities.

[WARNING]
====
This playbook exists for dev/testing purposes only!
====

|===

== Preparing the deployment

The OCP installation process requries the use of the OCP 
 pull secret. This secret can be obained from https://console.redhat.com/openshift/install/pull-secret.

As part of the installation process this information will be added 
 to the `install-config.yaml` and used in the OCP installation 
 process.

.Sample OCP pull secret JSON
[source,json]
----
{
  "auths": {
    "cloud.openshift.com": {"auth": "wwwwwwwwww", "email": "antcosta@redhat.com"}
    ,"quay.io": {"auth": "xxxxxxxxxxxxx", "email": "janedoe@example.com"}
    ,"registry.connect.redhat.com": {"auth": "yyyyyyyyyyyyy", "email": "janedoe@example.com"}
    ,"registry.redhat.io": {"auth": "zzzzzzzzzzzzzzzzz", "email": "janedoe@example.com"}
  }
}
----

[NOTE]
====
The presented examples will use the `OCP_PULL_SECRET` environment variable as a 
 holder for the OCP pull secret contents.
====

== Using a Bootstrap host

The default installation process uses the controller as the installation 
 source. This has several drawbacks amongst them is having to download, and 
 upload again, the OCP installer RHCOS image. Although downloading may take 
 some time, it is cached the first time it is executed, so it's the upload 
 task that takes the longest. To mitigate this problem the installation 
 process can be executed from a temporary host we call the bootstrap host.

To use the temporary boostrap host you have to create it first. The name of this RHOSP 
 Host can be any name but we suggest using the name of the cluster as prefix and 
 adding a suffix such as `-tmp-bootstrap-server`.

To create the boostrap host execute the RHOSP playbook created for that purpose.

.Sample execution of creating a bootstrap host.
[source,bash]
----
ansible-playbook ansible/playbook/openstack/openstack_vm_create_passwordstore.yml -e '{"openstack": {"vm": {"network": "provider_net_shared","image": "Fedora-Cloud-Base-37", "flavor": "m1.small"}}}' -e vm_name=ocp-xyz-tmp-bootstrap-server
----

After creating the bootstrap host execute the steps provided in the 
 <<deploy-ocp-on-rhos>> section. 

Once the OCP cluster is deployed you may remove the bootstrap server.

// tag::deploy_ocp_on_rhos[]
[#deploy-ocp-on-rhos]
== Deploy OCP Cluster on RHOS

The deployment playbook supports the follow variable entries.

.Script options
[%header,cols="25%,75%"]
|===
| Variable | Description

| `ocp_bootstrap_host`

[.fuchsia]#string#

a| VM name for the bootstrap host. 

If defined the installation process will be performed not on the 
 `localhost` controller but on the identified VM.

| `ocp_cluster_name`

[.fuchsia]#string# / [.red]#required# 

a| Name to be assigned to the OCP cluster

[NOTE]
====
Will also be applied as prefix to all the RHOS VM instances created as well 
 as other RHOS resources.
====

| `openshift_pull_secret`

[.fuchsia]#json# / [.red]#required# 

a| OCP pull secret for the user

| `ocp_root_directory`

[.fuchsia]#string# / [.red]#required# 

a| Root folder where for the installation. A new sub-folder with the 
 `ocp_cluster_name` name will be created and will serve as the 
 installation folder.

|===

Execute the playbook. Please note that this playbook uses `sudo` permission 
 to create several folders so the Ansible user must have `sudo` permission. 
 We're using the `-K` switch to ask for the `become` password which is only 
 required if the user as sudo permission with password. The folders created 
 will be associated (`uid:gid``) with the Ansible user used to connect to the 
 host.

[example]
----
ansible-playbook ansible/playbook/ocp/ocp_openstack_install.yml <1>
  -e ocp_root_directory=<2>
  -e ocp_cluster_name=<3>
  -e openshift_pull_secret=<4>
  -K <5>
----
<1> Playbook that implements the OCP deployment.
<2> Root directory for the installation.
<3> Name to be given to the cluster.
<4> OCP pull secret for the user.
<5> Ask for the become password.

.Command to execute the OCP deployment playbook
[source,bash]
----
ansible-playbook ansible/playbook/ocp/ocp_openstack_install.yml \
  -e ocp_root_directory=/opt/ocp \
  -e ocp_cluster_name=ocp-sdev \
  -e openshift_pull_secret=${OCP_PULL_SECRET} \
  -K
----

The playbook will result on the deployment of several RHOS VMs for control plane and worker nodes.

.Note on the RHOS VM flavors
[NOTE]
====
The RHOS flavors to be used on the VMs that will result on the OCP cluster are
defined by the `openstack_flavor_control_plane` and `openstack_flavor_compute` 
variables, having as default the values from the role defaults file.

.Ansible Role default flavor configuration
[source,yaml]
----
include::../../roles/ocp_cluster/defaults/main.yml[tag=rhos_default_flavors]
----

The list of flavors, and how to obtain them, is identified on the link:../../../openstack/README.adoc#Flavors[OpenStack README file].
====

The result of the deployment process is the following:

* OCP cluster deployed on RHOS instances as defined in the number and flavor of main and worker nodes
* RHOS instance that will serve as jump server to the OCP cluster
* Installation directory stored on the passwordstore and copied to the jump server
* OCP authentication information stored on the passwordstore
// end::deploy_ocp_on_rhos[]

[WARNING]
====
At this point the _bootstrap server_, if used, is no longer required.

[.lead]
Check that the installation folder is safely stored both on the jump server as well as on the local passwordstore before removing it.
====

// tag::undeploy_ocp_on_rhos[]
== Undeploy OCP Cluster on RHOS

[WARNING]
====
For the removal process to be successfull the OCP installation directory 
(`installation_dir`) must be provided as it store information on the RHOS
objects associated to the project.
====

.Command to execute the OCP cluster removal playbook.
[source,bash]
----
ansible-playbook -i inventory/ playbook/ocp/ocp_openstack_remove.yml \
  -e work_directory=/opt/ocp \
  -e installation_dir=/opt/ocp/openshift-data/
----
// end::undeploy_ocp_on_rhos[]

== Other OCP RHOS Playbooks

=== Get information from the OCP cluster

To collect information on the OCP cluster execute the 
 `ocp_openstack_info` playbook located at the `ansible/playbook/ocp/` 
 folder.


.Playbook parameters
[%header,cols="25%,75%"]
|===
| Variable | Description

| `ocp_root_directory`

[.fuchsia]#string# / [.red]#required# 

a| Root folder for the OCP installation.

Either define the `ocp_root_directory` and `ocp_cluster_name` variables 
 or the `installation_dir` one.

| `ocp_cluster_name`

[.fuchsia]#string# / [.red]#required# 

a| Name of the OCP cluster

Either define the `ocp_root_directory` and `ocp_cluster_name` variables 
 or the `installation_dir` one.

| `installation_dir`

[.fuchsia]#string#

a| Location of the installation directory.

Either define the `ocp_root_directory` and `ocp_cluster_name` variables 
 or the `installation_dir` one.


| `vm_name`

[.fuchsia]#string# / [.red]#required# 

a| Root folder where for the installation. A new sub-folder with the 
 `ocp_cluster_name` name will be created and will serve as the 
 installation folder.

|===

[source,bash]
----
ansible-playbook ansible/playbook/ocp/ocp_openstack_info.yml \
  -e ocp_root_directory=/opt/ocp \
  -e ocp_cluster_name=ocp-sdev \
  -e vm_name=ocp-sdev-zzzzz-jump-server -vv
----

